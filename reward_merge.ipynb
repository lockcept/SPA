{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "env_name = \"button-press-topdown-v2\"\n",
    "# env_name = \"box-close-v2\"\n",
    "exp_name = \"AESPA-17\"\n",
    "pair_algo = \"ternary-100\"\n",
    "reward_model_algo = \"MR-linear\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAJECTORY_LENGTH = 25\n",
    "\n",
    "import numpy as np\n",
    "from data_generation.utils import save_feedbacks_npz\n",
    "from data_loading.load_data import load_pair\n",
    "from data_loading.load_data import load_dataset\n",
    "from reward_learning.get_model import get_reward_model\n",
    "from utils.path import get_reward_model_path\n",
    "\n",
    "import csv\n",
    "import os\n",
    "\n",
    "save_dir = \"feedback_stats\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "csv_path = os.path.join(save_dir, f\"feedback_summary_{env_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "\n",
    "def predict_rewards(\n",
    "    env_name,\n",
    "    exp_name,\n",
    "    pair_algo,\n",
    "    reward_model_algo,\n",
    "):\n",
    "    dataset = load_dataset(env_name)\n",
    "    obs_dim = dataset[\"observations\"].shape[1]\n",
    "    act_dim = dataset[\"actions\"].shape[1]\n",
    "\n",
    "    print(\"obs_dim:\", obs_dim, \"act_dim:\", act_dim)\n",
    "    model_path_pattern = get_reward_model_path(\n",
    "        env_name=env_name,\n",
    "        exp_name=exp_name,\n",
    "        pair_algo=pair_algo,\n",
    "        reward_model_algo=reward_model_algo,\n",
    "        reward_model_tag=\"*\",\n",
    "    )\n",
    "    model_files = glob.glob(model_path_pattern)\n",
    "    model_list = []\n",
    "\n",
    "    for model_file in model_files:\n",
    "        model, _ = get_reward_model(\n",
    "            reward_model_algo=reward_model_algo,\n",
    "            obs_dim=obs_dim,\n",
    "            act_dim=act_dim,\n",
    "            model_path=model_file,\n",
    "            allow_existing=True,\n",
    "        )\n",
    "        model_list.append(model)\n",
    "\n",
    "    dataset = load_dataset(env_name)\n",
    "\n",
    "    num_samples = len(dataset[\"observations\"])\n",
    "    batch_size = num_samples // 20\n",
    "    model_outputs = []\n",
    "\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "\n",
    "        obs_batch = torch.tensor(\n",
    "            dataset[\"observations\"][start_idx:end_idx], dtype=torch.float32\n",
    "        ).to(device)\n",
    "        act_batch = torch.tensor(\n",
    "            dataset[\"actions\"][start_idx:end_idx], dtype=torch.float32\n",
    "        ).to(device)\n",
    "\n",
    "        batch_model_outputs = []\n",
    "        for model in model_list:\n",
    "            rewards = model.batched_forward_trajectory(\n",
    "                obs_batch=obs_batch, act_batch=act_batch\n",
    "            )\n",
    "            batch_model_outputs.append(rewards.detach().cpu().numpy())\n",
    "\n",
    "        batch_predicted_rewards = np.mean(batch_model_outputs, axis=0)\n",
    "        model_outputs.append(batch_predicted_rewards)\n",
    "\n",
    "    predicted_rewards = np.concatenate(model_outputs, axis=0).squeeze()\n",
    "\n",
    "    return predicted_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_feedback_from_raw_dataset(average_reward, cumulative_rewards, pairs):\n",
    "    \"\"\"\n",
    "    Fill feedback in dataset using cumulative rewards and calculate mu values.\n",
    "    \"\"\"\n",
    "\n",
    "    feedbacks = []\n",
    "\n",
    "    for s0, s1 in pairs:\n",
    "        sum_of_rewards_0 = cumulative_rewards[s0[1] - 1] - (\n",
    "            cumulative_rewards[s0[0] - 1] if s0[0] > 0 else 0\n",
    "        )\n",
    "        sum_of_rewards_1 = cumulative_rewards[s1[1] - 1] - (\n",
    "            cumulative_rewards[s1[0] - 1] if s1[0] > 0 else 0\n",
    "        )\n",
    "\n",
    "        if (\n",
    "            np.abs(sum_of_rewards_0 - sum_of_rewards_1)\n",
    "            < average_reward * TRAJECTORY_LENGTH * 0.1\n",
    "        ):\n",
    "            mu = 0.5\n",
    "        else:\n",
    "            mu = 0 if sum_of_rewards_0 > sum_of_rewards_1 else 1\n",
    "\n",
    "        feedbacks.append((s0, s1, mu))\n",
    "\n",
    "    return feedbacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_learning.train_model import train_reward_model\n",
    "\n",
    "\n",
    "feedback_count_to_add = 100\n",
    "\n",
    "raw_dataset = load_dataset(env_name)\n",
    "average_reward = np.mean(raw_dataset[\"rewards\"])\n",
    "cumulative_rewards = np.cumsum(raw_dataset[\"rewards\"])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(8, 10):\n",
    "    exp_name_with_suffix = f\"{exp_name}-{i:02d}\"\n",
    "    train_all_pairs_with_mu = load_pair(\n",
    "            env_name=env_name,\n",
    "            exp_name=exp_name_with_suffix,\n",
    "            pair_type=\"train_all\",\n",
    "            pair_algo=\"raw\",\n",
    "    )\n",
    "\n",
    "    all_traj_set = []\n",
    "\n",
    "    for p in train_all_pairs_with_mu:\n",
    "        all_traj_set.append(p[0])\n",
    "        all_traj_set.append(p[1])\n",
    "\n",
    "    for j in range(10):\n",
    "        if j == 0:\n",
    "            previous_pair_name = pair_algo\n",
    "        else:\n",
    "            previous_pair_name = current_pair_name\n",
    "\n",
    "        current_pair_name = f\"{pair_algo}-aug-high-{j:02d}\"\n",
    "\n",
    "        print(previous_pair_name)\n",
    "\n",
    "        predicted_rewards = predict_rewards(\n",
    "            env_name=env_name,\n",
    "            exp_name=exp_name_with_suffix,\n",
    "            pair_algo=previous_pair_name,\n",
    "            reward_model_algo=reward_model_algo,\n",
    "        )\n",
    "\n",
    "        predicted_rewards = np.array(predicted_rewards)\n",
    "        predicted_cumsum = np.cumsum(predicted_rewards)\n",
    "\n",
    "        # ✅ sub_traj + 원본 traj 인덱스 저장\n",
    "        traj_with_reward = []\n",
    "        for traj_idx, traj in enumerate(all_traj_set):\n",
    "            for start in range(traj[0], traj[1] - TRAJECTORY_LENGTH + 1):\n",
    "                traj_start = start\n",
    "                traj_end = start + TRAJECTORY_LENGTH\n",
    "\n",
    "                reward_sum = predicted_cumsum[traj_end - 1] - (\n",
    "                    predicted_cumsum[traj_start - 1] if traj_start > 0 else 0\n",
    "                )\n",
    "                sub_traj = (traj_start, traj_end)\n",
    "                traj_with_reward.append((sub_traj, reward_sum, traj_idx))   \n",
    "\n",
    "        traj_with_reward.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        pairs = []\n",
    "        num_pairs = feedback_count_to_add\n",
    "        # stride = len(traj_with_reward) / (num_pairs * 2)\n",
    "\n",
    "        # for i in range(num_pairs):\n",
    "        #     idx1 = int(i * 2 * stride)\n",
    "        #     idx2 = idx1 + 1\n",
    "        #     if idx2 >= len(traj_with_reward):\n",
    "        #         break\n",
    "\n",
    "        #     traj1 = traj_with_reward[idx1][0]\n",
    "        #     traj2 = traj_with_reward[idx2][0]\n",
    "        #     pairs.append((traj1, traj2))\n",
    "\n",
    "        # for i in range(num_pairs):\n",
    "        #     idx1 = i\n",
    "        #     idx2 = len(traj_with_reward) - i - 1\n",
    "\n",
    "        #     traj1 = traj_with_reward[idx1][0]\n",
    "        #     traj2 = traj_with_reward[idx2][0]\n",
    "        #     pairs.append((traj1, traj2))\n",
    "\n",
    "        \n",
    "\n",
    "        k = 0\n",
    "        while k < len(traj_with_reward) - 1:\n",
    "            traj1, _, idx1 = traj_with_reward[k]\n",
    "            traj2, _, idx2 = traj_with_reward[k + 1]\n",
    "\n",
    "            if idx1 != idx2:\n",
    "                pairs.append((traj1, traj2))\n",
    "                k += 2  \n",
    "            else:\n",
    "                k += 1  \n",
    "\n",
    "            if len(pairs) >= feedback_count_to_add:\n",
    "                break\n",
    "        \n",
    "        feedbacks = fill_feedback_from_raw_dataset(\n",
    "            average_reward=average_reward,\n",
    "            cumulative_rewards=cumulative_rewards,\n",
    "            pairs=pairs,\n",
    "        )\n",
    "\n",
    "        count_0 = 0\n",
    "        count_0_5 = 0\n",
    "        count_1 = 0\n",
    "\n",
    "        for _, _, mu in feedbacks:\n",
    "            if mu == 0:\n",
    "                count_0 += 1\n",
    "            elif mu == 0.5:\n",
    "                count_0_5 += 1\n",
    "            elif mu == 1:\n",
    "                count_1 += 1\n",
    "\n",
    "        write_header = not os.path.exists(csv_path)\n",
    "\n",
    "        with open(csv_path, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "\n",
    "            if write_header:\n",
    "                writer.writerow([\"exp_name\", \"pair_name\", \"feedback_count\", \"mu=0\", \"mu=0.5\", \"mu=1\"])\n",
    "\n",
    "            writer.writerow([\n",
    "                exp_name_with_suffix,\n",
    "                current_pair_name,\n",
    "                len(feedbacks),\n",
    "                count_0,\n",
    "                count_0_5,\n",
    "                count_1\n",
    "            ])\n",
    "        \n",
    "        # add expected feedbacks\n",
    "        for idx, (s0, s1) in enumerate(pairs):\n",
    "            low_traj = traj_with_reward[len(traj_with_reward) - 1 - idx]\n",
    "\n",
    "            feedbacks.append(\n",
    "                (low_traj[0], s0, 1)\n",
    "            )\n",
    "            feedbacks.append(\n",
    "                (low_traj[0], s1, 1)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        previous_feedback = load_pair(\n",
    "            env_name=env_name,\n",
    "            exp_name=exp_name_with_suffix,\n",
    "            pair_type=\"train\",\n",
    "            pair_algo=previous_pair_name,\n",
    "        ).tolist()\n",
    "\n",
    "        print(len(previous_feedback+feedbacks))\n",
    "\n",
    "        save_feedbacks_npz(\n",
    "            env_name=env_name,\n",
    "            exp_name=exp_name_with_suffix,\n",
    "            feedbacks=previous_feedback + feedbacks,\n",
    "            pair_type=\"train\",\n",
    "            pair_name=current_pair_name\n",
    "        )\n",
    "\n",
    "        for k in range(7):\n",
    "            train_reward_model(\n",
    "                env_name=env_name,\n",
    "                exp_name=exp_name_with_suffix,\n",
    "                pair_algo=current_pair_name,\n",
    "                reward_model_algo=reward_model_algo,\n",
    "                reward_model_tag=f\"{k:02d}\",\n",
    "                num_epoch=100,\n",
    "        )\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "dataset = load_dataset(env_name)\n",
    "\n",
    "save_dir = \"reward_eval_stats\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "csv_path = os.path.join(save_dir, \"reward_eval_summary.csv\")\n",
    "\n",
    "write_header = not os.path.exists(csv_path)\n",
    "if write_header:\n",
    "    with open(csv_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"env_name\", \"exp_name\", \"pair_algo\", \"pcc\", \"order_agreement\"])\n",
    "\n",
    "for i in range(10):\n",
    "    exp_name_with_suffix = f\"{exp_name}-{i:02d}\"\n",
    "    for j in range(10):\n",
    "        current_pair_algo = f\"ternary-100-aug-similar-{j:02d}\"\n",
    "        predicted_rewards = predict_rewards(\n",
    "            env_name=env_name,\n",
    "            exp_name=exp_name_with_suffix,\n",
    "            pair_algo=current_pair_algo,\n",
    "            reward_model_algo=reward_model_algo,\n",
    "        )\n",
    "\n",
    "        true_rewards = dataset[\"rewards\"]\n",
    "\n",
    "        true_rewards = np.array(true_rewards).flatten()\n",
    "        predicted_rewards = np.array(predicted_rewards).flatten()\n",
    "\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        plt.scatter(true_rewards, predicted_rewards, alpha=0.1, s=10, label=\"Samples\")\n",
    "        plt.xlabel(\"True Reward\")\n",
    "        plt.ylabel(\"Predicted Reward\")\n",
    "        plt.title(f\"Predicted vs True Rewards ({exp_name_with_suffix} / {current_pair_algo})\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        try:\n",
    "            pcc = np.corrcoef(true_rewards, predicted_rewards)[0, 1]\n",
    "        except Exception as e:\n",
    "            print(f\"[{exp_name_with_suffix}] PCC 계산 중 오류 발생: {e}\")\n",
    "            pcc = float('nan')\n",
    "\n",
    "        num_samples = 100000\n",
    "        agree = 0\n",
    "        n = len(true_rewards)\n",
    "\n",
    "        for _ in range(num_samples):\n",
    "            i1, i2 = random.sample(range(n), 2)\n",
    "            gt_diff = true_rewards[i1] - true_rewards[i2]\n",
    "            pred_diff = predicted_rewards[i1] - predicted_rewards[i2]\n",
    "\n",
    "            if gt_diff * pred_diff > 0 or (gt_diff == 0 and pred_diff == 0):\n",
    "                agree += 1\n",
    "\n",
    "        order_agreement = agree / num_samples\n",
    "\n",
    "        with open(csv_path, mode=\"a\", newline=\"\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\n",
    "                env_name,\n",
    "                exp_name_with_suffix,\n",
    "                current_pair_algo,\n",
    "                f\"{pcc:.6f}\",\n",
    "                f\"{order_agreement:.6f}\",\n",
    "            ])\n",
    "\n",
    "        print(f\"[{exp_name_with_suffix}]\")\n",
    "        print(f\"  PCC (Pearson Correlation): {pcc:.4f}\")\n",
    "        print(f\"  Order Agreement ({num_samples} pairs): {order_agreement:.4f}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lockcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
