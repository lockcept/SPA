{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"__file__\"), \"src\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# env_name = \"button-press-topdown-v2\"\n",
    "env_name = \"box-close-v2\"\n",
    "exp_name = \"AESPA-20-00\"\n",
    "pair_algo = \"ternary-500\"\n",
    "reward_model_algo = \"MR-dropout\"\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\" \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "TRAJECTORY_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generation.data_research import test4\n",
    "from data_generation.picker.mr_dropout import mr_dropout_test\n",
    "\n",
    "\n",
    "data = test4(\n",
    "    env_name=env_name,\n",
    "    exp_name=exp_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "\n",
    "def analyze_prediction_uncertainty(data, env_name=\"env\"):\n",
    "    # 1. 데이터 분리\n",
    "    data = np.array(data)  # shape [T, 3]\n",
    "    true_rewards = data[:, 0].squeeze()\n",
    "    mean_rewards = data[:, 1].squeeze()\n",
    "    std_rewards = data[:, 2].squeeze()\n",
    "\n",
    "    # 2. uncertainty 기준 4분위수 나누기\n",
    "    quantiles = np.percentile(std_rewards, [25, 50, 75])\n",
    "    q1, q2, q3 = quantiles\n",
    "    print(f\"Quantiles: Q1={q1:.3f}, Q2={q2:.3f}, Q3={q3:.3f}\")\n",
    "\n",
    "    groups = {\n",
    "        \"Q1 (lowest 25%)\": np.where(std_rewards <= q1)[0],\n",
    "        \"Q2 (25-50%)\": np.where((std_rewards > q1) & (std_rewards <= q2))[0],\n",
    "        \"Q3 (50-75%)\": np.where((std_rewards > q2) & (std_rewards <= q3))[0],\n",
    "        \"Q4 (highest 25%)\": np.where(std_rewards > q3)[0],\n",
    "    }\n",
    "\n",
    "    # 3. pairwise acc 계산 함수\n",
    "    def compute_pairwise_accuracy(true_vals, pred_vals, pair_count=100000):\n",
    "        n = len(true_vals)\n",
    "        correct = 0\n",
    "\n",
    "        for _ in range(pair_count):\n",
    "            i, j = random.sample(range(n), 2)\n",
    "            true_cmp = true_vals[i] > true_vals[j]\n",
    "            pred_cmp = pred_vals[i] > pred_vals[j]\n",
    "            if true_cmp == pred_cmp:\n",
    "                correct += 1\n",
    "\n",
    "        return correct / pair_count\n",
    "\n",
    "    # 4. plot\n",
    "    plt.figure(figsize=(14, 10))\n",
    "\n",
    "    for i, (name, indices) in enumerate(groups.items(), 1):\n",
    "        tr = true_rewards[indices]\n",
    "        pr = mean_rewards[indices]\n",
    "\n",
    "        if np.all(tr == tr[0]) or np.all(pr == pr[0]):\n",
    "            pcc = float('nan')\n",
    "        else:\n",
    "            pcc, _ = pearsonr(tr, pr)\n",
    "\n",
    "        acc = compute_pairwise_accuracy(tr, pr)\n",
    "\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.scatter(tr, pr, alpha=0.1, label=f'PCC={pcc:.3f}, Acc={acc:.3f}')\n",
    "        plt.xlabel(\"True Rewards\")\n",
    "        plt.ylabel(\"Predicted Rewards\")\n",
    "        plt.title(f\"{name} (n={len(indices)})\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Reward Prediction ({env_name}): Uncertainty Groups (Scatter, PCC, Pairwise Accuracy)\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "    # 5~7: 통합 서브플롯\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "\n",
    "    # 서브플롯 #5: Predicted vs Uncertainty\n",
    "    axes[0].scatter(mean_rewards, std_rewards, alpha=0.1)\n",
    "    axes[0].set_xlabel(\"Predicted Rewards\")\n",
    "    axes[0].set_ylabel(\"Predicted Uncertainty (Std)\")\n",
    "    axes[0].set_title(\"Predicted vs. Uncertainty\")\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # 샘플 추출\n",
    "    sample_size = 2000\n",
    "    total_samples = len(true_rewards)\n",
    "    if sample_size >= total_samples:\n",
    "        sample_indices = np.arange(total_samples)\n",
    "    else:\n",
    "        sample_indices = np.random.choice(total_samples, size=sample_size, replace=False)\n",
    "\n",
    "    tr_sample = true_rewards[sample_indices]\n",
    "    pr_sample = mean_rewards[sample_indices]\n",
    "    std_sample = std_rewards[sample_indices]\n",
    "\n",
    "    # 서브플롯 #6: True vs Pred (색: uncertainty)\n",
    "    sc = axes[1].scatter(tr_sample, pr_sample, c=std_sample, cmap=\"viridis\", alpha=0.6)\n",
    "    axes[1].set_xlabel(\"True Rewards\")\n",
    "    axes[1].set_ylabel(\"Predicted Rewards\")\n",
    "    axes[1].set_title(\"True vs. Predicted (Color = Uncertainty)\")\n",
    "    axes[1].grid(True)\n",
    "    fig.colorbar(sc, ax=axes[1], label=\"Predicted Uncertainty (Std)\")\n",
    "\n",
    "    # 서브플롯 #7: True vs Uncertainty\n",
    "    axes[2].scatter(true_rewards, std_rewards, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"True Rewards\")\n",
    "    axes[2].set_ylabel(\"Predicted Uncertainty (Std)\")\n",
    "    axes[2].set_title(\"True vs. Uncertainty\")\n",
    "    axes[2].grid(True)\n",
    "\n",
    "    plt.suptitle(f\"Reward Model Prediction({env_name}): Uncertainty Relationships\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_prediction_uncertainty(data, env_name=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading.load_data import load_pair\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from random import sample\n",
    "\n",
    "def generate_confident_traj_pairs(all_trajs, top_k=100000,num_buckets=20):\n",
    "    z = 10\n",
    "    confident_pairs = []\n",
    "\n",
    "    # Step 1: 정렬 & bucket 나누기 (mean 기준)\n",
    "    sorted_trajs = sorted(enumerate(all_trajs), key=lambda x: x[1][2])  # sort by mean_reward\n",
    "    total = len(sorted_trajs)\n",
    "\n",
    "    buckets = [\n",
    "        sorted_trajs[ total * i // num_buckets : total * (i + 1) // num_buckets ]\n",
    "        for i in range(num_buckets)\n",
    "    ]\n",
    "\n",
    "    # Step 2: bucket 간 confident pair 탐색\n",
    "    pairs_per_bucket = top_k // ((num_buckets * (num_buckets - 1)) // 2)\n",
    "\n",
    "    for i in range(num_buckets):\n",
    "        for j in range(i + 1, num_buckets):\n",
    "            trajs_i = buckets[i]\n",
    "            trajs_j = buckets[j]\n",
    "            local_confident_pairs = []\n",
    "\n",
    "            for idx_i, traj_i in trajs_i:\n",
    "                mu_i, std_i = traj_i[2], traj_i[3]\n",
    "                upper_i = mu_i + z * std_i\n",
    "\n",
    "                for idx_j, traj_j in trajs_j:\n",
    "                    mu_j, std_j = traj_j[2], traj_j[3]\n",
    "                    lower_j = mu_j - z * std_j\n",
    "\n",
    "                    if upper_i < lower_j:\n",
    "                        # j가 i보다 확실히 우수 → pair 추가\n",
    "                        local_confident_pairs.append((traj_i, traj_j))\n",
    "\n",
    "            if len(local_confident_pairs) > pairs_per_bucket:\n",
    "                confident_pairs.extend(sample(local_confident_pairs, pairs_per_bucket))\n",
    "            else:\n",
    "                confident_pairs.extend(local_confident_pairs)\n",
    "\n",
    "            if len(confident_pairs) >= top_k:\n",
    "                return confident_pairs[:top_k]\n",
    "\n",
    "    return confident_pairs[:top_k]\n",
    "\n",
    "\n",
    "def plot_traj_scatter_by_uncertainty_bins(all_trajs, num_bins=6):\n",
    "    uncertainties = np.array([traj[3] for traj in all_trajs])\n",
    "    bins = np.percentile(uncertainties, np.linspace(0, 100, num_bins + 1))\n",
    "    vmin, vmax = uncertainties.min(), uncertainties.max()\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 9))\n",
    "    gs = gridspec.GridSpec(2, 4, width_ratios=[1, 1, 1, 0.05])  # 마지막 column은 colorbar\n",
    "\n",
    "    axes = [fig.add_subplot(gs[i // 3, i % 3]) for i in range(num_bins)]\n",
    "\n",
    "    for i in range(num_bins):\n",
    "        bin_lower = bins[i]\n",
    "        bin_upper = bins[i + 1]\n",
    "        ax = axes[i]\n",
    "\n",
    "        bin_trajs = [traj for traj in all_trajs if bin_lower <= traj[3] <= bin_upper]\n",
    "        true_rewards = [traj[1] for traj in bin_trajs]\n",
    "        predicted_rewards = [traj[2] for traj in bin_trajs]\n",
    "        stds = [traj[3] for traj in bin_trajs]\n",
    "\n",
    "        sc = ax.scatter(\n",
    "            true_rewards,\n",
    "            predicted_rewards,\n",
    "            c=stds,\n",
    "            cmap='viridis',\n",
    "            alpha=0.4,\n",
    "            vmin=vmin,\n",
    "            vmax=vmax,\n",
    "        )\n",
    "\n",
    "        ax.set_title(f\"Bin {i+1}: [{bin_lower:.3f}, {bin_upper:.3f}]\")\n",
    "        ax.set_xlabel(\"True Reward\")\n",
    "        ax.set_ylabel(\"Predicted Reward\")\n",
    "        ax.grid(True)\n",
    "\n",
    "    # 컬러바는 gridspec의 마지막 열에 따로 넣기\n",
    "    cbar_ax = fig.add_subplot(gs[:, 3])\n",
    "    cbar = fig.colorbar(sc, cax=cbar_ax)\n",
    "    cbar.set_label(\"Uncertainty (std)\")\n",
    "\n",
    "    fig.suptitle(\"Trajectory-wise Reward Scatter by Uncertainty Bins\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 0.96, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "def eval_test(data, env_name):\n",
    "    test_feedbacks = load_pair(\n",
    "        env_name=env_name,\n",
    "        exp_name=exp_name,\n",
    "        pair_type=\"test\",\n",
    "        pair_algo=\"ternary-100000\",\n",
    "    )\n",
    "\n",
    "    data = np.array(data)  # shape [T, 3]\n",
    "    true_rewards = data[:, 0].squeeze()\n",
    "    mean_rewards = data[:, 1].squeeze()\n",
    "    std_rewards = data[:, 2].squeeze()\n",
    "    var_rewards = std_rewards ** 2\n",
    "\n",
    "    true_rewards_cum = np.cumsum(true_rewards, dtype=np.float64)\n",
    "    mean_rewards_cum = np.cumsum(mean_rewards, dtype=np.float64)\n",
    "    var_rewards_cum = np.cumsum(var_rewards, dtype=np.float64)\n",
    "\n",
    "    correct_uncertainties = []\n",
    "    incorrect_uncertainties = []\n",
    "\n",
    "    all_trajs = []\n",
    "\n",
    "    for i, (idx0, idx1, _) in enumerate(test_feedbacks):\n",
    "        s0, e0 = idx0\n",
    "        s1, e1 = idx1\n",
    "\n",
    "        true_reward_sum_0 = true_rewards_cum[e0 - 1] - (true_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        true_reward_sum_1 = true_rewards_cum[e1 - 1] - (true_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        mean_reward_sum_0 = mean_rewards_cum[e0 - 1] - (mean_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        mean_reward_sum_1 = mean_rewards_cum[e1 - 1] - (mean_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        var_rewards_sum_0 = var_rewards_cum[e0 - 1] - (var_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        var_rewards_sum_1 = var_rewards_cum[e1 - 1] - (var_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "        std_reward_sum_0 = np.sqrt(var_rewards_sum_0)\n",
    "        std_reward_sum_1 = np.sqrt(var_rewards_sum_1)\n",
    "\n",
    "        true_reward_bigger = true_reward_sum_0 < true_reward_sum_1\n",
    "        mean_reward_bigger = mean_reward_sum_0 < mean_reward_sum_1\n",
    "\n",
    "        if true_reward_bigger != mean_reward_bigger:\n",
    "            incorrect_uncertainties.append(std_reward_sum_0)\n",
    "            incorrect_uncertainties.append(std_reward_sum_1)\n",
    "        else:\n",
    "            correct_uncertainties.append(std_reward_sum_0)\n",
    "            correct_uncertainties.append(std_reward_sum_1)\n",
    "        \n",
    "        all_trajs.append(((s0, e0), true_reward_sum_0, mean_reward_sum_0, std_reward_sum_0))\n",
    "        all_trajs.append(((s1, e1), true_reward_sum_1, mean_reward_sum_1, std_reward_sum_1))\n",
    "\n",
    "    correct_uncertainties = np.array(correct_uncertainties)\n",
    "    incorrect_uncertainties = np.array(incorrect_uncertainties)\n",
    "\n",
    "    print (f\"Correct Uncertainties: {len(correct_uncertainties)}\")\n",
    "    print (f\"Incorrect Uncertainties: {len(incorrect_uncertainties)}\")\n",
    "    print (f\"Mean Correct Uncertainties: {np.mean(correct_uncertainties):.3f}\")\n",
    "    print (f\"Mean Incorrect Uncertainties: {np.mean(incorrect_uncertainties):.3f}\")\n",
    "\n",
    "    # true reward가 양수인 trajectory만 필터링\n",
    "    filtered_trajs = [traj for traj in all_trajs if traj[1] >= 0]\n",
    "\n",
    "    # 상위 100개 certainty(low uncertainty) 추출\n",
    "    all_trajs_sorted = sorted(filtered_trajs, key=lambda x: x[3])\n",
    "    top_100_trajs = all_trajs_sorted[:100]\n",
    "\n",
    "    from itertools import combinations\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for traj1, traj2 in combinations(top_100_trajs, 2):\n",
    "        _, true1, mean1, _ = traj1\n",
    "        _, true2, mean2, _ = traj2\n",
    "        if (true1 > true2) and (mean1 > mean2):\n",
    "            correct += 1\n",
    "        elif (true1 < true2) and (mean1 < mean2):\n",
    "            correct += 1\n",
    "        elif (true1 == true2) and (mean1 == mean2):\n",
    "            correct += 1\n",
    "\n",
    "        total += 1\n",
    "\n",
    "    print(f\"[Top-100 Traj Pair Accuracy] {correct}/{total} = {correct/total:.3f}\")\n",
    "\n",
    "    # 조건: 예측 보상 차이가 불확실성 합의 n배보다 큰 경우만 필터링\n",
    "    confident_feedbacks = []\n",
    "\n",
    "    for i, (idx0, idx1, _) in enumerate(test_feedbacks):\n",
    "        s0, e0 = idx0\n",
    "        s1, e1 = idx1\n",
    "\n",
    "        true_reward_sum_0 = true_rewards_cum[e0 - 1] - (true_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        true_reward_sum_1 = true_rewards_cum[e1 - 1] - (true_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        mean_reward_sum_0 = mean_rewards_cum[e0 - 1] - (mean_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        mean_reward_sum_1 = mean_rewards_cum[e1 - 1] - (mean_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        var_rewards_sum_0 = var_rewards_cum[e0 - 1] - (var_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        var_rewards_sum_1 = var_rewards_cum[e1 - 1] - (var_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "        std_reward_sum_0 = np.sqrt(var_rewards_sum_0)\n",
    "        std_reward_sum_1 = np.sqrt(var_rewards_sum_1)\n",
    "\n",
    "        reward_diff = abs(mean_reward_sum_0 - mean_reward_sum_1)\n",
    "        uncertainty_sum = std_reward_sum_0 + std_reward_sum_1\n",
    "\n",
    "        if reward_diff > 10 * uncertainty_sum:\n",
    "            confident_feedbacks.append((\n",
    "                true_reward_sum_0, true_reward_sum_1,\n",
    "                mean_reward_sum_0, mean_reward_sum_1\n",
    "            ))\n",
    "\n",
    "    # 정확도 계산\n",
    "    total = len(confident_feedbacks)\n",
    "    correct = 0\n",
    "    for true0, true1, pred0, pred1 in confident_feedbacks:\n",
    "        if (true0 > true1 and pred0 > pred1) or \\\n",
    "           (true0 < true1 and pred0 < pred1) or \\\n",
    "           (true0 == true1 and pred0 == pred1):\n",
    "            correct += 1\n",
    "\n",
    "    if total > 0:\n",
    "        print(f\"[Confident Pair Accuracy] {correct}/{total} = {correct/total:.3f}\")\n",
    "    else:\n",
    "        print(\"No confident pairs found with the given threshold.\")\n",
    "\n",
    "    # Bradley-Terry 모델을 사용하여 승리 확률 계산 (Baseline)\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    bt_confident_feedbacks = []\n",
    "\n",
    "    for i, (idx0, idx1, _) in enumerate(test_feedbacks):\n",
    "        s0, e0 = idx0\n",
    "        s1, e1 = idx1\n",
    "\n",
    "        true_reward_sum_0 = true_rewards_cum[e0 - 1] - (true_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        true_reward_sum_1 = true_rewards_cum[e1 - 1] - (true_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        mean_reward_sum_0 = mean_rewards_cum[e0 - 1] - (mean_rewards_cum[s0 - 1] if s0 > 0 else 0)\n",
    "        mean_reward_sum_1 = mean_rewards_cum[e1 - 1] - (mean_rewards_cum[s1 - 1] if s1 > 0 else 0)\n",
    "\n",
    "        # Bradley-Terry 승리 확률 (0이 1을 이길 확률)\n",
    "        bt_prob = sigmoid(mean_reward_sum_0 - mean_reward_sum_1)\n",
    "\n",
    "        if bt_prob <= 0.001 or bt_prob >= 0.999:\n",
    "            bt_confident_feedbacks.append((\n",
    "                true_reward_sum_0, true_reward_sum_1,\n",
    "                mean_reward_sum_0, mean_reward_sum_1,\n",
    "                bt_prob\n",
    "            ))\n",
    "\n",
    "    # 정확도 계산\n",
    "    total = len(bt_confident_feedbacks)\n",
    "    correct = 0\n",
    "    for true0, true1, pred0, pred1, _ in bt_confident_feedbacks:\n",
    "        if (true0 > true1 and pred0 > pred1) or \\\n",
    "           (true0 < true1 and pred0 < pred1) or \\\n",
    "           (true0 == true1 and pred0 == pred1):\n",
    "            correct += 1\n",
    "\n",
    "    if total > 0:\n",
    "        print(f\"[BT Prob (0.001 or 0.999) Accuracy] {correct}/{total} = {correct/total:.3f}\")\n",
    "    else:\n",
    "        print(\"No highly confident BT pairs found.\")\n",
    "\n",
    "    # 그룹으로 나누어 Augmentation\n",
    "    confident_pairs = generate_confident_traj_pairs(all_trajs[:20000], top_k=100000)\n",
    "\n",
    "    total = len(confident_pairs)\n",
    "    correct = 0\n",
    "\n",
    "    for traj_i, traj_j in confident_pairs:\n",
    "        true_i = traj_i[1]\n",
    "        true_j = traj_j[1]\n",
    "\n",
    "        pred_i = traj_i[2]\n",
    "        pred_j = traj_j[2]\n",
    "\n",
    "\n",
    "        if (true_i > true_j and pred_i > pred_j) or \\\n",
    "            (true_i < true_j and pred_i < pred_j) or \\\n",
    "            (true_i == true_j and pred_i == pred_j):\n",
    "                correct += 1\n",
    "\n",
    "    if total > 0:\n",
    "        print(f\"[Generated Confident Pair Accuracy] {correct}/{total} = {correct/total:.3f}\")\n",
    "    else:\n",
    "        print(\"No confident trajectory pairs generated.\")\n",
    "\n",
    "\n",
    "    plot_traj_scatter_by_uncertainty_bins(all_trajs)\n",
    "\n",
    "    # Mean Reward vs. Std Scatter Plot\n",
    "    mean_rewards_for_plot = [traj[2] for traj in all_trajs]\n",
    "    std_rewards_for_plot = [traj[3] for traj in all_trajs]\n",
    "\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    plt.scatter(mean_rewards_for_plot, std_rewards_for_plot, alpha=0.4, c='blue')\n",
    "    plt.xlabel(\"Mean Predicted Reward\")\n",
    "    plt.ylabel(\"Uncertainty (std)\")\n",
    "    plt.title(\"Mean Reward vs. Uncertainty (Trajectory-wise)\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ----- BT 기반 -----\n",
    "    bt_pred_0 = [p[2] for p in bt_confident_feedbacks]\n",
    "    bt_pred_1 = [p[3] for p in bt_confident_feedbacks]\n",
    "    bt_true_0 = [p[0] for p in bt_confident_feedbacks]\n",
    "    bt_true_1 = [p[1] for p in bt_confident_feedbacks]\n",
    "\n",
    "    # ----- generate_confident_traj_pairs 기반 (랜덤 뒤집기 포함) -----\n",
    "    pred_i = []\n",
    "    pred_j = []\n",
    "    true_i = []\n",
    "    true_j = []\n",
    "\n",
    "    for j, i in confident_pairs:\n",
    "        if random.random() < 0.5:\n",
    "            i, j = j, i  # swap\n",
    "        pred_i.append(i[2])\n",
    "        pred_j.append(j[2])\n",
    "        true_i.append(i[1])\n",
    "        true_j.append(j[1])\n",
    "\n",
    "    # ----- 시각화 (2x2 subplot) -----\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "    # 1. BT 기반 predicted scatter\n",
    "    axes[0, 0].scatter(bt_pred_0, bt_pred_1, alpha=0.4, c='green')\n",
    "    axes[0, 0].set_xlabel(\"Predicted Reward 0\")\n",
    "    axes[0, 0].set_ylabel(\"Predicted Reward 1\")\n",
    "    axes[0, 0].set_title(\"BT Prob - Predicted Reward\")\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # 2. Confident Pair predicted scatter\n",
    "    axes[0, 1].scatter(pred_i, pred_j, alpha=0.4, c='blue')\n",
    "    axes[0, 1].set_xlabel(\"Predicted Reward (i)\")\n",
    "    axes[0, 1].set_ylabel(\"Predicted Reward (j)\")\n",
    "    axes[0, 1].set_title(\"Confident Pair - Predicted Reward\")\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # 3. BT 기반 true scatter\n",
    "    axes[1, 0].scatter(bt_true_0, bt_true_1, alpha=0.4, c='orange')\n",
    "    axes[1, 0].set_xlabel(\"True Reward 0\")\n",
    "    axes[1, 0].set_ylabel(\"True Reward 1\")\n",
    "    axes[1, 0].set_title(\"BT Prob - True Reward\")\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # 4. Confident Pair true scatter\n",
    "    axes[1, 1].scatter(true_i, true_j, alpha=0.4, c='purple')\n",
    "    axes[1, 1].set_xlabel(\"True Reward (i)\")\n",
    "    axes[1, 1].set_ylabel(\"True Reward (j)\")\n",
    "    axes[1, 1].set_title(\"Confident Pair - True Reward\")\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_test(data, env_name=env_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lockcept",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
